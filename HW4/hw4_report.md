## HW 4 Report

# 1. Implementation
## 1.a Describe how you implemented the FlashAttention forward pass using CUDA.
**Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.**

**(1) Matrix Blocking**  
- å°‡Â QÂ åˆ†ç‚ºÂ BRÃ—BKÂ çš„å€å¡Šï¼Œæ¯å€‹ thread è² è²¬éƒ¨åˆ†è¼‰å…¥è³‡æ–™ã€‚
è¨ˆç®—å…¬å¼åŠå°æ‡‰çš„ç¨‹å¼ç¢¼å¦‚ä¸‹ï¼š
![Attention Formula](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Implementation-math-1.png?raw=true)

```cpp
// è¼‰å…¥ Q çš„åˆ†å¡Šåˆ° shared memory
int qStart = batchOffset + blockRow * BR * dim; 
for (int i = threadIdFlat; i < BR * BK; i += numThreadsPerTile) {
    int row = i / BK; // ç¢ºå®šåœ¨åˆ†å¡Šå…§çš„ row ç´¢å¼•
    int col = i % BK; // ç¢ºå®šåœ¨åˆ†å¡Šå…§çš„ column ç´¢å¼•
    if (row < BR && col < dim) {
        qTile[row][col] = d_Q[qStart + row * dim + col];
    } else {
        qTile[row][col] = 0.0f; // è¶…å‡ºç¯„åœè£œé›¶
    }
}
```

**(2) è¨ˆç®— attention scores ä¸¦ä½¿ç”¨ scaling factors**
- QÂ çš„æ¯ä¸€ row èˆ‡Â KÂ çš„æ¯ä¸€ column ç›¸ä¹˜ï¼Œå½¢æˆ attention scores ã€‚
- ä½¿ç”¨ scaling factorsÂ Â é¿å… overflowï¼Œæå‡ç©©å®šæ€§ã€‚
è¨ˆç®—å…¬å¼åŠå°æ‡‰çš„ç¨‹å¼ç¢¼å¦‚ä¸‹ï¼š
![Attention Formula](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Implementation-math-2.png?raw=true)

```cpp
// è¨ˆç®— Q * K^Tï¼Œä¸¦é€æ­¥ç´¯åŠ 
for (int d = 0; d < BK; d++) {
    ...
    for (int i = 0; i < THREAD_ROWS; i++) {
        for (int j = 0; j < THREAD_COLS; j++) {
            // æ¯å€‹ thread è² è²¬è¨ˆç®—éƒ¨åˆ†çš„ Q * K^T 
            threadResults[i * THREAD_COLS + j] += qVals[i] * kVals[j];
        }
    }
}
// é€²è¡Œç¸®æ”¾
float scaling = 1.0f / sqrtf((float)dim);
for (int i = 0; i < THREAD_ROWS; i++) {
    for (int j = 0; j < THREAD_COLS; j++) {
        // æ¯å€‹ thread è² è²¬è¨ˆç®—éƒ¨åˆ†çš„ Q * K^T å¾Œçš„ scaling
        threadResults[i * THREAD_COLS + j] *= scaling;
    }
}
```

**(3)  Softmax è¨ˆç®— (é¿å…åˆ†æ¯ç‚º0)**
- Softmax çš„è¨ˆç®—å®¹æ˜“å› æ•¸å€¼éå¤§å°è‡´ overflow çš„å•é¡Œï¼Œé€éæ¸›å»æœ€å¤§å€¼ (row-wise max) è§£æ±ºæ•¸å€¼ä¸ç©©å®šæ€§ã€‚å…·é«”çš„åšæ³•æ˜¯ï¼š
- `rowMax[i]`Â å­˜ç›®å‰ row çš„æœ€å¤§å€¼ï¼Œé¿å…æŒ‡æ•¸é‹ç®—ä¸­å‡ºç¾ overflowã€‚
- `outAcc` ç´¯ç© softmax åˆ†å­ï¼Œä¸¦ä¹˜ä¸Š Vã€‚
- `rowSumExp[i]`Â ç´¯ç© softmax åˆ†æ¯ã€‚
è¨ˆç®—å…¬å¼åŠå°æ‡‰çš„ç¨‹å¼ç¢¼å¦‚ä¸‹ï¼š
![Attention Formula](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Implementation-math-3.png?raw=true)

```cpp
#pragma unroll
for (int i = 0; i < THREAD_ROWS; i++) {
    int row = tileRowIdx * THREAD_ROWS + i;
    float currentMax = rowMax[i];
    for (int j = 0; j < BC; j++) {
        currentMax = fmaxf(currentMax, scoreTile[row][j]);  // æ›´æ–°æœ€å¤§å€¼ (row-wise max)
    }
    rowMax[i] = currentMax;
}
// è¨ˆç®— Softmax
for (int i = 0; i < THREAD_ROWS; i++) {
    for (int j = 0; j < BC; j++) {
        float scoreVal = scoreTile[row][j];
        float p = expf(scoreVal - rowMax[i]); // æ¸›å»æœ€å¤§å€¼
        rowSumExp[i] += p; // ç´¯ç©è¨ˆç®— softamx æ‰€éœ€è¦çš„åˆ†æ¯
        for (int k = 0; k < BK; k++) {
            outAcc[i][k] += p * vTile[j][k]; // åˆ†å­çš„éƒ¨åˆ†å…ˆä¹˜ä¸Šå°æ‡‰çš„ V
        }
    }
}
```

**(4) å®Œæˆ softmax ä¸¦å¯«å›å…¨å±€è¨˜æ†¶é«” O**
- å°‡ `outAcc` é™¤ä»¥è¨ˆç®—å®Œçš„ `rowSumExp` å¾—åˆ° softmax å¾Œçš„ Q * K^T * V çš„æœ€çµ‚ç­”æ¡ˆã€‚
- å°‡æœ€çµ‚çµæœå¯«å›å…¨åŸŸè¨˜æ†¶é«” Oã€‚
è¨ˆç®—å…¬å¼åŠå°æ‡‰çš„ç¨‹å¼ç¢¼å¦‚ä¸‹ï¼š
![Attention Formula](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Implementation-math-4.png?raw=true)

```cpp
// å°‡ outAcc é€²è¡Œæ¨™æº–åŒ–ä¸¦å¯«å›å…¨å±€è¨˜æ†¶é«”
for (int i = 0; i < THREAD_ROWS; i++) {
    float invSumExp = 1.0f / rowSumExp[i];
    for (int k = 0; k < BK; k++) {
        if (k < dim) {
            // é™¤ä»¥ä¸Šä¸€éƒ¨æ‰€è¨ˆç®—å‡ºçš„åˆ†æ¯ï¼ˆå³å®Œæˆ softmax çš„è¨ˆç®—ï¼‰
            d_O[oStart + row * dim + k] = outAcc[i][k] * invSumExp; 
        }
    }
}
```

---

## 1.b Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
- æˆ‘å°‡ Qã€Kã€V åˆ†å¡Šç‚º BRÃ—BC çš„å€å¡Šï¼Œä½¿æ¯å€‹ CUDA block èƒ½å¤ åŒæ™‚è² è²¬ä¸€éƒ¨åˆ†çš„ row èˆ‡ columnã€‚é€™æ¨£å°±å¯ä»¥åœ¨æœ‰é™çš„ shared memory ç©ºé–“ä¸­è™•ç†ç¶­åº¦ d çš„ç‰‡æ®µã€‚
ä¸‹åœ–æ˜¯å…·é«”çš„åˆ†é…æ–¹å¼çš„ç¤ºæ„åœ–ï¼š
![Division](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Implementation-division-1.png?raw=true)

---

## 1.c Describe how you chose the block sizes B_r and B_c and why.
æˆ‘é¸æ“‡ `BR=BC=8`ï¼Œé€™æ˜¯åšå®Œå¯¦é©—å¾Œç™¼ç¾æœ€ä½³çš„é…ç½®ã€‚
ç•¶ `BK=64` æ™‚ï¼Œ`qTile` å’Œ `kTile` çš„è¨˜æ†¶é«”éœ€æ±‚åˆ†åˆ¥ç‚º `BRÃ—BK` å’Œ `BCÃ—BK`ã€‚è‹¥é¸æ“‡ `BR=BC=32`ï¼Œå…±äº«è¨˜æ†¶é«”çš„å£“åŠ›æœƒå¤§å¹…å¢åŠ ï¼Œå¯èƒ½å°è‡´ block ç„¡æ³•åŒæ™‚åŸ·è¡Œï¼Œé™ä½å¹³è¡Œåº¦ã€‚æ­¤å¤–ï¼Œ`BR=BC=8` è®“æ¯å€‹ CUDA block è™•ç†è¼ƒå°ç¯„åœçš„è³‡æ–™ï¼Œèƒ½åˆ†é…çµ¦æ›´å¤š block åŒæ™‚åŸ·è¡Œï¼Œæå‡ GPU çš„å¹³è¡Œé‹ç®—æ•ˆèƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ`BR=BC=32` é›–ç„¶æ¸›å°‘äº† block ç¸½æ•¸ï¼Œä½†æ¯å€‹ block å ç”¨æ›´å¤šè³‡æºï¼Œå¯èƒ½å°è‡´ GPU æ ¸å¿ƒé–’ç½®ï¼Œç„¡æ³•å……åˆ†ç™¼æ®ç¡¬é«”çš„èƒ½åŠ›ã€‚å› æ­¤ `BR=BC=8` çš„è¨­å®šæˆ–è¨±æ¯”è¼ƒèƒ½åœ¨è¨ˆç®—èˆ‡è¨˜æ†¶é«”è¨ªå•é–“å–å¾—æœ€ä½³å¹³è¡¡ï¼ŒåŒæ™‚ç¢ºä¿è³‡æºåˆ©ç”¨ç‡æœ€å¤§åŒ–ã€‚

---

## 1.d Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
- Threads per block å’Œ grid dimensions çš„é…ç½®
    - æ¯å€‹ block æœƒåˆ†åˆ° `(BC/THREAD_COLS)* (R/THREAD_ROWS)` å€‹ threadsï¼Œå› ç‚ºæ¯å€‹ thread æœƒè² è²¬è™•ç† `THREAD_COLS*THREAD_ROWS` å€‹å…ƒç´ 
    - æ¯å€‹ grid æœƒæœ‰ `(batchSize)*((seqLen+BR-1)/BR)` å€‹ blocks, å…¶ä¸­æ ¹æ“š BR çš„å¤§å°ä¾†æ±ºå®šè¦å°‡ seqLen åˆ†æˆå¹¾å¡Šã€‚

    ```CPP
    dim3 blockDim(BC / THREAD_COLS, BR / THREAD_ROWS);
    dim3 gridDim(batchSize, (seqLen + BR - 1) / BR);
    ```

- Shared memory allocation çš„é…ç½®
    - æ¯å€‹ Q, K, V çš„ column é•·åº¦éƒ½æ˜¯ BK ï¼Œè² è²¬è™•ç†æ¯å€‹ç¶­åº¦çš„è¨ˆç®—ã€‚
    - Q ä¸€æ¬¡æœƒè™•ç† BR å€‹ rows, è€Œ K, V æœƒè™•ç† BC å€‹ rowsã€‚
    - ç”±æ–¼ O å­˜çš„æ˜¯ç›¸ä¹˜å¾Œçš„çµæœï¼Œåˆå› ç‚º (BRÃ—BK) * (BKÃ—BC) = (BRÃ—BC)ï¼Œæ‰€ä»¥ O çš„é…ç½®æ˜¯ BRÃ—BCã€‚
    ```cpp
    // ä½¿ç”¨ shared memory å„²å­˜åˆ†å¡Šå¾Œçš„ Q, K, V, ä»¥åŠ O
    __shared__ float qTile[BR][BK + 1]; // Q
    __shared__ float kTile[BC][BK + 1];  // K
    __shared__ float vTile[BC][BK]; // V
    __shared__ float scoreTile[BR][BC + 1]; // O
    ```
---

## 1.e Justify your choices and how they relate to the blocking factors and the SRAM size.
æˆ‘æ˜¯æ ¹æ“šå¯¦é©—çµæœä¾†é¸æ“‡BRå’ŒBCçš„å¤§å°ï¼Œè€Œ `BK`ï¼Œæ˜¯ç‚º dimension æœ€å¤šæ˜¯ 64ï¼Œæ‰€ä»¥è¨­å®šæˆ64ã€‚åœ¨å¯¦é©—ä¸­ï¼Œç•¶åˆ†å¡Šå¤§å°å¾Â `BR=BC=16`Â æ¸›å°è‡³Â `BR=BC=8Â `æ™‚ï¼ŒåŸ·è¡Œæ™‚é–“å¾ `4.475` ç§’é™è‡³ `3.526` ç§’ï¼Œè¡¨ç¤ºå‡ºè¼ƒå°çš„åˆ†å¡Šè¨­è¨ˆèƒ½é¡¯è‘—æå‡æ•ˆèƒ½ã€‚é€™å¯èƒ½æ˜¯å› ç‚ºÂ `BR=BC=8`Â çš„åˆ†å¡Šå¤§å°èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å…±äº«è¨˜æ†¶é«”ï¼Œåœ¨é¿å…å…±äº«è¨˜æ†¶é«”å£“åŠ›çš„åŒæ™‚ï¼Œå…è¨±æ›´å¤š CUDA blocks å¹³è¡Œé‹è¡Œï¼Œé€²ä¸€æ­¥æå‡ GPU è³‡æºçš„åˆ©ç”¨ç‡ã€‚

æ­¤å¤–ï¼Œé€™æ¨£çš„è¨­è¨ˆä¹Ÿå¯ä»¥æ¸›å°‘äº† thread é–“çš„åŒæ­¥èˆ‡è³‡æºç«¶çˆ­å•é¡Œï¼Œæé«˜æ•´é«”æ•ˆç‡ã€‚å¾ä¸‹é¢çš„å¯¦é©—çµæœä¸­å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº† Block Size èª¿æ•´å¤–ï¼Œå…¶ä»–æŠ€è¡“ï¼ˆå¦‚ Tiling å’Œè§£æ±º Bank Conflictï¼‰ä¹Ÿå°æ•ˆèƒ½æœ‰é€²ä¸€æ­¥å„ªåŒ–ä½œç”¨ï¼Œä½† Block Size çš„èª¿æ•´æ˜¯æå‡æ•ˆèƒ½çš„æœ€é—œéµå› ç´ ä¹‹ä¸€ï¼Œå……åˆ†å±•ç¤ºäº†é€™æ˜¯ä¸éŒ¯çš„åˆ†å¡Šç­–ç•¥ï¼

---

# 2. Profiling Results
ä¸‹é¢åˆ†åˆ¥ç‚ºé‡é»æ‘˜è¦çš„å’Œè©³ç´°çš„è¼¸å‡ºçµæœï¼š

| Metric Name                 | Value           |
| -------------------------   | --------------- |
| **Achieved Occupancy**      | **12.5%**       |
| **SM Efficiency**           | **99.41%**      |
| **Shared Load Throughput**  | **1528.0 GB/s** |
| **Shared Store Throughput** | **13.956 GB/s** |
| **Global Load Throughput**  | **4.03 GB/s**   |
| **Global Store Throughput** | **81.002 MB/s** |

```bash
Profiling results for kernel: flash_attention_kernel
Profiling metric: achieved_occupancy
==3423229== NVPROF is profiling process 3423229, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 1.560 seconds
==3423229== Profiling application: ./hw4 testcases/t28 t28.out
==3423229== Profiling result:
==3423229== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                        achieved_occupancy                        Achieved Occupancy    0.125000    0.125000    0.125000
-------------------------------
Profiling metric: sm_efficiency
==505524== NVPROF is profiling process 505524, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 1.558 seconds
==505524== Profiling application: ./hw4 testcases/t28 t28.out
==505524== Profiling result:
==505524== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                             sm_efficiency                   Multiprocessor Activity      99.41%      99.41%      99.41%
-------------------------------
Profiling metric: shared_load_throughput
==3423305== NVPROF is profiling process 3423305, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 1.558 seconds
==3423305== Profiling application: ./hw4 testcases/t28 t28.out
==3423305== Profiling result:
==3423305== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                    shared_load_throughput             Shared Memory Load Throughput  1528.0GB/s  1528.0GB/s  1528.0GB/s
-------------------------------
Profiling metric: shared_store_throughput
==794295== NVPROF is profiling process 794295, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 1.585 seconds
==794295== Profiling application: ./hw4 testcases/t28 t28.out
==794295== Profiling result:
==794295== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                   shared_store_throughput            Shared Memory Store Throughput  13.956GB/s  13.956GB/s  13.956GB/s
-------------------------------
Profiling metric: gld_throughput
==3423343== NVPROF is profiling process 3423343, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 3.991 seconds
==3423343== Profiling application: ./hw4 testcases/t28 t28.out
==3423343== Profiling result:
==3423343== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                            gld_throughput                    Global Load Throughput  4.0298GB/s  4.0298GB/s  4.0298GB/s
-------------------------------
Profiling metric: gst_throughput
==1576528== NVPROF is profiling process 1576528, command: ./hw4 testcases/t28 t28.out
B: 4, N: 16384, d: 64
(B, N, d): (4, 16384, 64)
Time: 1.590 seconds
==1576528== Profiling application: ./hw4 testcases/t28 t28.out
==1576528== Profiling result:
==1576528== Metric result:
Invocations                               Metric Name                        Metric Description         Min         Max         Avg
Device "NVIDIA GeForce GTX 1080 (0)"
    Kernel: flash_attention_kernel(float*, float*, float*, float*, int, int, int)
          1                            gst_throughput                   Global Store Throughput  81.002MB/s  81.002MB/s  81.002MB/s
-------------------------------

```
---

# 3. Experiment & Analysis
## 3.a System Spec 
- ä½¿ç”¨çš„æ˜¯ Apollo-GPU server

---

## 3.b Optimization
æˆ‘ä½¿ç”¨ `t29` é€™ç­†æ¸¬è³‡ä¾†åšå¯¦é©—ï¼Œå› ç‚ºä»–æ˜¯ hw4-judge çš„æ‰€æœ‰æ¸¬è³‡ä¸­èŠ±æœ€å¤šæ™‚é–“åŸ·è¡Œçš„ã€‚
| Optimization Technique | Execution Time (s) |
| ---                    | ---                |
| GPU Baseline           | 4.475 seconds      |
| Block Size 16â†’8        | 3.526 seconds      |  
| Tiling                 | 3.376 seconds      |
| Bank Conflict          | 2.961 seconds      |
| Unroll                 | 2.897 seconds      |

![Optimization](https://github.com/ChienPei/Parallel-Programming-HW4/blob/main/pics/Optimization.png?raw=true)

æˆ‘ä½¿ç”¨äº†ç¸®å° block sizeã€tilingã€unrollã€coalesced memory accessã€shared memoryã€handle bank conflict å’Œ cuda 2d alignment é€™å¹¾å€‹å„ªåŒ–æ–¹æ³•ã€‚åœ–ä¸­åˆ—å‡º block sizeã€tilingã€unrollï¼Œå¯ä»¥ç™¼ç¾ç¢ºå¯¦æœ‰å„ªåŒ–æ•ˆæœã€‚é€™äº›å„ªåŒ–æŠ€è¡“å…±åŒå°‡åŸ·è¡Œæ™‚é–“å¾ baseline çš„ 4.475 ç§’ç¸®çŸ­è‡³æœ€ä½³çš„ 2.897 ç§’ï¼Œæ•´é«”æ•ˆèƒ½æå‡ç´„ 35%ã€‚

---


# 4. Experience & conclusion
## 4.a What have you learned from this homework?
åœ¨é€™æ¬¡ä½œæ¥­ä¸­ï¼Œæˆ‘å­¸åˆ°äº†ä»¥ä¸‹å¹¾é»ï¼š
1. æ·±å…¥ç†è§£ FlashAttention é‹ä½œï¼š
äº†è§£äº†çŸ©é™£åˆ†å¡Šã€Softmax æ•¸å€¼ç©©å®šæ€§è™•ç†ï¼Œä»¥åŠå…±äº«è¨˜æ†¶é«”åœ¨æå‡æ•ˆèƒ½ä¸­çš„é‡è¦è§’è‰²ã€‚
2. æ•¸å€¼ç©©å®šæ€§èˆ‡æ•ˆèƒ½æå‡ï¼š
ä½¿ç”¨ row-wise æœ€å¤§å€¼è§£æ±º Softmax é‹ç®—ä¸­çš„ overflow å•é¡Œï¼Œä¸¦é€é tilingã€unroll ç­‰æŠ€è¡“é€²ä¸€æ­¥å„ªåŒ–é‹ç®—æ•ˆç‡ã€‚
3. CUDA ç¨‹å¼è¨­è¨ˆå¯¦ä½œï¼š
å­¸ç¿’åˆ°å¦‚ä½•é¸æ“‡åˆé©çš„ block sizeï¼Œæ­é…å…±äº«è¨˜æ†¶é«”æœ€ä½³åŒ–ï¼Œé”æˆ GPU è³‡æºæœ€å¤§åŒ–åˆ©ç”¨ã€‚
4. çµè«–ï¼š
é€™æ¬¡ä½œæ¥­è®“æˆ‘å° CUDA çš„è¨­è¨ˆèˆ‡æ•ˆèƒ½å„ªåŒ–æœ‰æ›´æ·±å…¥çš„ç†è§£ï¼Œé€éå¯¦ä½œèˆ‡åˆ†ææœ‰æ•ˆæå‡ GPU çš„å¹³è¡Œè¨ˆç®—èƒ½åŠ›ï¼Œé€™å°æœªä¾†è™•ç†é«˜æ•ˆèƒ½é‹ç®—æœ‰å¾ˆå¤§çš„å¹«åŠ©ï¼